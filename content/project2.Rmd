---
title: "Mansa Prasad Project 2"
output:
  html_document: default
  pdf_document: default
---

***

<br><br>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```

# Introduction

I decided to work with halloween candy data for this project since it seemed fun, it's about what kind of candy people would like to recieve during halloween. This data set has 13 variables, most of which are categorical variables that are already sorted with 0's and 1's to represent if they do or don't have the variable listed. They surveyed 85 types of candy, thus there are 85 observations. The categorical variable that I'm particularly interested in are *chocolate*,  I might also look into some of the other categorical vaireables in addition to the main ones I'm focusing on. There are three numerical variables in this dataset, *sugarpecent*, which is the percentile of sugar it falls under within the data set, *pricepercent*, which is the unit price percentile compared to the rest of the set, and *winpercent*, which is the overall win percentage according to 269,000 votes. I was a little unclear on the win percent variable so I looked futher into it and figured out it was how often that particular candy was chosen as the prefered candy to recieve during halloween. 

***

### Packages

```{r}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lmtest)
library(glmnet)
library(sandwich)
library(plotROC)
library(vegan)
```

***

<br>

# DataSet:

This is the halloween candy dataset that I used!
```{r}
candy <- read.csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/candy-power-ranking/candy-data.csv")

glimpse(candy)

```

***

<br>

# MANOVA:

For the MANOVA I'm checking the sugar percentage and the price percentage of the halloween candies based if the candy has chocolate in it or not.

```{r}
# Creating a MANOVA variable
candyman <- manova(cbind(sugarpercent, pricepercent)~chocolate, data = candy)
summary(candyman)

# Checking mean differences compared to the categorical variable 
candy %>%
  group_by(chocolate) %>%
  summarise(mean(sugarpercent), mean(pricepercent))

# Univariate ANOVA based on the MANOVA object. 
summary.aov(candyman)

# Post-hoc t test 
pairwise.t.test(candy$pricepercent, candy$chocolate, p.adj = "none")

```

My data that I chose to perform a MANOVA with doesn't have that big of a mean difference between the variables, but I chose to run a univariate ANOVAs just to make sure that the means really aren't too differnt. There was at least one significant difference among the the candy being chocolate or not for at least one of the dependent variables. When the univariate ANOVA was run, the variable that had a significant differnce was the price percent variable where the p < 0.05. When the post hoc analysis was performed conducting pairwise comparisons to determine which chocolate prefernce differed in the price percent it showed me that they both differ signigicantly in terms of price percent.

***

<br>

## Randomization test

I had to run a PERMANOVA as my randomization test since my original data was from an MANOVA so this was the most logical choice. For this I'm comparing the chocolate variable to the sugar and price percent depending on if the candy is chocolate or not!

```{r}
# Do a PERMANOVA
sugaMoni <- candy %>%
  select(sugarpercent, pricepercent) %>%
  dist()

adonis(sugaMoni~chocolate, data=candy)

#compute observed F
SST <- sum(sugaMoni^2)/85

SSW <- candy%>%
  group_by(chocolate)%>%
  select(sugarpercent, pricepercent)%>%
  do(d = dist(.[1:2],"euclidean"))%>% 
  ungroup()%>%
  summarize(sum(d[[1]]^2)/50 + 
            sum(d[[2]]^2)/50)%>% 
  pull

F_obs<-((SST-SSW)/1)/(SSW/83) #observed F statistic


# compute null distribution for F
Fs <- replicate(1000,{
new <- candy %>%
  mutate(chocolate=sample(chocolate)) 

SSW <- new %>%
  group_by(chocolate) %>%
  select(sugarpercent, pricepercent) %>%
  do(d = dist(.[1:2],"euclidean")) %>%
  ungroup() %>%
  summarize(sum(d[[1]]^2)/50 + sum(d[[2]]^2)/50) %>%
  pull

((SST-SSW)/1)/(SSW/83)

})

{hist(Fs,prob = T); abline(v=F_obs, col="red", add=T)}

# p- value
mean(Fs>F_obs)
```

H0: mean sugar % for chocolate = mean sugar % for non chocolate = mean price % for chocolate = mean price % for non chocolate
HA: at least one of these means differs from the others
The results of this test shows me that I get the same conclution as my MANOVA results and we reject the null hypothesis since the p value is so small.

***

<br>

# Linear Regression Model

<br>

Using the candy data set I'm going to compare the unite price of the Halloween candy to the preference of each candy, based on if it's chocolate or not, chosen by the people who took this survey to see if there is a linear relationship. My response variable will be the price, thus my regression will be predicting the *pricePercent* from the *winPercent* and *chocolate* variables. 

```{r}
# Mean centering the data
candy$winpercent_c <- candy$winpercent - mean(candy$winpercent)
candy$pricepercent_c <- candy$pricepercent - mean(candy$pricepercent)

# Linear model
candylm <- lm(pricepercent_c ~ winpercent_c + chocolate, 
              data = candy)
summary(candylm)

# Plotting the regression

candy$chocolate1 <-factor(candy$chocolate,
                        levels=c("1","0"))

candy %>%
  ggplot(aes(pricepercent_c, winpercent_c)) +
  geom_point(aes(color = chocolate1)) +
  geom_smooth(method = "lm", se = F, aes(color=chocolate1)) +
  ggtitle("Mean Centered Unit Price vs Preference of Chocolate for Halloween") +
  xlab("Price Percentile") +
  ylab("Preference Percentile") 
```

<br>

As seen in this plot above there is a lot of information being thrown around! first we can take a look at the linear regression table where it shows us that the intercept coefficient is -0.119, which lets us know that when the price point is 0 the preference of candy that people have is 0 which makes logical sense. People can't want something if there's nothing offered. The coefficient for win percent which is at 0.0007 basically meaning there is no increased preference of candy type based on price. On the other hand, the coefficient for chocolate was 0.27, meaning that the preference of chocolate candy's increase the price of those chocolates also increase! 
It's really interesting to see how there is a stonger preference (win percent) for chocolate as the type of candy being recieved for halloween compared to non-chocolate candy's. We can also see that based on the particular chocolates chosen the price also increases in comparison to the other candy's. Based on this information we can see that there is a correlation between the mean prefernece of chocolate candy and their mean cost for this halloween survey.

***

<br>

```{r}
resids <- candylm$residuals
candyfits <- candylm$fitted.values

# Normality check with Shapiro-Wilk test
shapiro.test(resids)

# Histogram for normality
ggplot()+geom_histogram(aes(resids),bins=20)

# Checking variance with residual values

ggplot()+
  geom_point(aes(candyfits, resids)) +
  geom_hline(yintercept = 0, color = "red")


```

<br>

In this case the p-value from the Shapiro-Wilk is greater thatn 0.05 which would indicate that this relationship does follow normality, and this is backed up with the histogram which shows an ached bellcurve type shape, proving that this relationship is pretty normal. The residual plot allows us to see homoskedaticity which apprears as two groups which represent chocolate vs non-chocolate candy, both which are ok and follow homoskedaticity. 

***

<br>

```{r}
# Robust standard errors
bptest(candylm)
coeftest(candylm, vcov = vcovHC(candylm))
```

According to the Breusch-Pagan test normality is met since the p-value is barely over 0.05 meaning there is a significance differnce in the variables. When looking at the coefficients for the robust standard error we can tell that there is not much of a difference from the uncorrected SE values! The uncorrected values were: 0.041(intercept), 0.0024(winpercent), and 0.07(chocolate), the corrected values were:0.038(intercept), 0.0028(winpercent), and 0.08(chocolate).

Additionally, the proportion of the variation explained by the outcome from my model was 0.2375 which means that the price isn't really explaining the variation of the win percent or chocolate regardless of significance.

***

<br>

## Bootstrapping

```{r}
# Creating a variable that repeat 5000 times
samp_distn<-replicate(5000, {
  boot_candy <- sample_frac(candy, replace=T) 
  candylm1 <- lm(pricepercent_c~winpercent_c + chocolate1, data = boot_candy) 
  coef(candylm1) 
})

samp_distn %>% t %>% as.data.frame %>% summarize_all(sd)
```

The bootstrapped SE values are differnt from the roburst and original values, the intercept value for the bootstrap is 0.058 which is higher than the other two which were 0.41 and 0.38, the mean adjusted win percent is 0.0027 which is close to the roburst value which was 0.0028, and the chocolate value is basically in between the other two at 0.079.

***

<br>

# Logistic Regression

### Predicting Binary Category

```{r}
candyfit <- glm(chocolate~sugarpercent+pricepercent, 
                data = candy, 
                family = binomial(link = "logit"))
summary(candyfit)

exp(coef(candyfit))

probs <- predict(candyfit, type = "response")

```

 <br>

The coefficients of candyfit shows the log odds of the variables that are being compared to chocolate. We can take the exponentiate coeffecients to look at the odds ratios. This shows me that at the lowest sugar and price percent the odds of the candy being chocolate is at 0.117. For sugarpercent when we control the price percent, the percent increase results the odds of the candy being chocolate increase by a factor of 0.51. For pricepercent when we control the sugar percent, the percent increase results the odds of the candy being chocolate increase by a factor of 95.42.

***

<br>

```{r}
# Confution matrix
table(truth = candy$chocolate,
      prediction = as.numeric(probs> 0.5)) %>%
  addmargins()
```


```{r}

class_diag <- function(probs,truth){
  tab <- table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc = sum(diag(tab))/sum(tab)
  sens = tab[2,2]/colSums(tab)[2]
  spec = tab[1,1]/colSums(tab)[1]
  ppv = tab[2,2]/rowSums(tab)[2]
  if(is.numeric(truth) == FALSE & is.logical(truth)==FALSE)
    truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord <- order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR = cumsum(truth)/max(1,sum(truth)) 
  FPR = cumsum(!truth)/max(1,sum(!truth))
  
  dup <- c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR <- c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc <- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}

class_diag(probs, candy$chocolate)
```

The accuracy is the proportion of correctly sorted candy based on if they're chocolate or not with a probability greated than 0.5, my accuracy was 0.76.
TPR the proportion of correctly sorted candy that are not chocolates with a probability greated than 0.5, my TPR was 0.67.
TNR the proportion of correctly sorted candy that are chocolates with a probability greated than 0.5, my TNR was 0.833.
The recall is the proportion of candy sorted as non chocolates with a probability greated than 0.5 that that actually are! My precsion was 0.75

<br>

The coefficients 

***

<br>

```{r}
# Density plot
candy$logit <- predict(candyfit, type = "link")

candy %>%
  ggplot() +
  geom_density(aes(logit, color = chocolate1, fill = chocolate1), alpha=.4) +
  theme(legend.position = c(.92,.90)) +
  geom_vline(xintercept = 0) +
  xlab("logit (log-odds)") +
  geom_rug(aes(logit, color = chocolate1))

# ROC curve
candy$prob <- predict(candyfit, type = "response")

ROCplot <- ggplot(candy) +
  geom_roc(aes(d = chocolate, m = prob), n.cuts=0) +
  ggtitle("ROC curve based on Chocolate trends")

ROCplot
calc_auc(ROCplot)
```

<br>

My ROC curve is pretty good, there is not too many FPR (false positives) there is a good amount of area under the curve, my AUC based on TPR and FPR is 0.802 which is a good value!

***

<br>

```{R}
set.seed(1234)
k=10

# 10 fold process
candydat <- candy[sample(nrow(candy)), ]
folds <- cut(seq(1:nrow(candy)), 
             breaks = k,
             labels = F)
diags <- NULL

for (i in 1:k) {
  train <- candydat[folds!=i, ] 
  test <- candydat[folds==i, ]  
  truth <- test$chocolate
  
  candyfit <- glm(chocolate~sugarpercent+pricepercent, 
                data = candy, 
                family = binomial(link = "logit"))
  probs <- predict(candyfit, newdata = test, type = "response")
  diags <- rbind(diags, class_diag(probs, truth))
}

summarise_all(diags, mean)
```

After preforming the 10 fold CV the acc, sens, spec, and ppv didn't chance much and the AUC went up by a little bit, but nothing too significant. The accurary stayed the same, the sensitivity when from 0.67 to 0.70 and the recall went from 0.76 to 0.73.

<br>
 
### Lasso regression

```{r}
set.seed(1234)

# matix prep
y <- as.matrix(candy$chocolate1) 
x <- model.matrix(chocolate1 ~ ., data = candy)[, -1] 
cv <- cv.glmnet(x, y, family = "binomial")

# lasso
lasso <- glmnet(x, y, family = "binomial", lambda = cv$lambda.1se) 
coef(lasso)

set.seed(1234)
k=10

data <- candy %>% 
  sample_frac 
folds <- ntile(1:nrow(data),n=10) 

diags<-NULL
for(i in 1:k){
  train <- data[folds!=i,] 
  test <- data[folds==i,] 
  truth <- test$chocolate 
  
  fit <- glm(chocolate~sugarpercent+pricepercent, 
                data = candy, 
                family = binomial(link = "logit"))
  probs <- predict(candyfit, newdata=test, type="response")
  
  diags<-rbind(diags,class_diag(probs,truth))
}

diags %>% summarize_all(mean)

```

This part was a little confusing for me, I did the lasso regression but my only variable that my only predicted variable is chocolate, thus I just chose to run the 10 fold CV with my previous glm. When I did this I saw that there was a small change in the AUC, it became a little higher! The accuracy stayed the same as all the previous calculations I'm assuming that's because my data didn't have much overfitting for these variables! 




